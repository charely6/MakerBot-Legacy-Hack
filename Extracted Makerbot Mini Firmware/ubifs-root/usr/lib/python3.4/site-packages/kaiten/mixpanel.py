import base64
import collections
import datetime
import logging
import logging.handlers
import subprocess
import glob
import json
import os

import kaiten.log
import kaiten.constants

class MixPanel:
    """ Class for sending MixPanel events to watchtower """
    def __init__(self, server, config):
        self._log = kaiten.log.getlogger(self)
        self._server = server
        self.reload_config(config)
        self._normal_duration = datetime.timedelta(minutes=10)
        self._contract_duration = self._normal_duration
        # Note that cache entries are already base64 encoded
        self._cache = collections.deque()
        self._disk_cache = CacheFileLogger(self._analytics_dropped_callback)
        self._sending_cache = None
        self._sending_cache_file = None
        # _cache_to_disk set to True so that an upload attempt of the cache
        # will occur on bootup (if online)
        self._cache_to_disk = True
        self._generator = self._next()

    def reload_config(self, config):
        self._config = config['kaiten']['mixpanel']

    def contract_duration(self):
        return self._contract_duration

    def expected_run_time(self):
        return datetime.timedelta(seconds=kaiten.constants.normal_generator_time)

    def event(self, event, **properties):
        """ Adds an event to our event cache """
        properties['token'] = self._config['mixpanel_token']
        data = {'event': event, 'properties': properties}
        json_data = json.dumps(data).encode('utf-8')
        # Add a trailing comma and pad to a multiple of 3 bytes
        json_data += b' ' * (2 - len(json_data) % 3) + b','
        self._cache.append(base64.urlsafe_b64encode(json_data))

    def _analytics_dropped_callback(self):
        self._server.mixpanel_event("analytics_events_dropped")

    def _change_contract_duration(self, duration):
        self._contract_duration = duration
        self._server.reschedule_contract_generator(self)

    def _batch_success(self, response):
        # TODO: The mixpanel API specifies that we should check if the
        # response is "0" or "1" to determine success, but this is not
        # what watchtower does.
        if self._sending_cache_file:
            # we are sending files from disk, delete the file since we succeeded
            os.remove(self._sending_cache_file)
            self._sending_cache_file = None
        self._sending_cache = None

    def _batch_error(self, **kwargs):
        self._log.error('Error sending analytics (%r), Will cache to disk', kwargs)
        if self._sending_cache_file is None:
            # this cache was sent from memory not disk so write it
            for e in self._sending_cache:
                self._disk_cache.append(e)
        self._cache_to_disk = True
        self._sending_cache_file = None
        self._sending_cache = None

    def _load_next_disk_cache(self, path):
        cache = collections.deque()
        # shorten the contract duration during file reading
        self._change_contract_duration(datetime.timedelta(milliseconds=500))
        with open(path) as lfp:
            for line in lfp.readlines():
                cache.extend([bytes(line.strip(), 'utf-8')])
                yield
        self._change_contract_duration(self._normal_duration)
        if len(cache):
            self._sending_cache = cache

    def _send_cache(self):
        # if there is something on the disk start sending the cache files
        # and save anything in the cache to disk
        # Set aside everything cached so far to be sent asynchronously
        if self._cache_to_disk:
            # cache anything in memory to disk and send one of the logs
            yield from self._write_to_disk()
            fp = self._disk_cache.get_oldest_log_path()
            if fp.endswith(kaiten.constants.analytics_cache):
                # don't try to send the currently open file
                self._disk_cache._handler.doRollover()
                fp = fp + ".1"
            self._sending_cache_file = fp
            yield from self._load_next_disk_cache(fp)
        elif len(self._cache):
            # Send cache from memory
            self._sending_cache = self._cache
            self._cache = collections.deque()
        if self._sending_cache is None:
            # nothing to send, don't spam
            # we've emptied the buffer, don't cache anything else to disk
            if self._sending_cache_file:
                if os.path.exists(self._sending_cache_file):
                    """This happens when we try to call send cache after having
                    been writing to disk, then emptying the saved files,
                    when we try to send the currently open file (see above)
                    and that file is empty so self._sending_cache == None
                    """
                    os.remove(self._sending_cache_file)
                self._sending_cache_file = None
            self._cache_to_disk = False
            return
        body = CacheReader(self._sending_cache)
        self._server.http_request(
            self._config['mixpanel_url'], '/track/', 'POST', body,
            success_callback=self._batch_success,
            error_callback=self._batch_error,
            encoding='url', timeout=10)

    def _write_to_disk(self):
        if len(self._cache):
            self._change_contract_duration(datetime.timedelta(milliseconds=500))
            cache_to_save = self._cache
            self._cache = collections.deque()
            for e in cache_to_save:
                self._disk_cache.append(e)
                yield
            self._change_contract_duration(self._normal_duration)

    def __next__(self):
        return next(self._generator)

    def _next(self):
        while True:
            if self._server.is_online() and self._sending_cache is None:
                # we're online and there isn't already a cache being sent
                yield from self._send_cache()
            elif len(self._cache):
                # We are not online or there's a file already being sent
                self._cache_to_disk = True
                yield from self._write_to_disk()
            # if neither are true there are no events to send
            yield

class CacheFileLogger():
    """
    Generic cache writer, handles writing to memory or to disk
    """
    def __init__(self, data_lost_callback=None):
        self._log_path = kaiten.constants.analytics_cache
        self._logger = logging.getLogger('CacheFileLogger')
        self._logger.setLevel(logging.INFO)
        self._data_lost_callback = data_lost_callback
        self._init_handler()

    def _init_handler(self):
        self._handler = RotatingCacheHandler(
            self._log_path, mode='ab',
            maxBytes=24*1024, backupCount=256,
            encoding='utf-8',
            data_lost_callback=self._data_lost_callback)
        self._logger.addHandler(self._handler)

    def clear(self):
        #Close all handlers in preparation for log deletion and reopening
        handlers = list(self._logger.handlers)
        for h in handlers:
            self._logger.removeHandler(h)
            h.flush()
            h.close()
            yield
        #Remove all log files
        for lp in self.get_log_paths():
            os.remove(lp)
            yield
        #Create a new handler with a clean log file
        self._init_handler()

    def append(self, string):
        self._logger.info(str(string, 'utf-8'))

    def get_oldest_log_path(self):
        return sorted(self.get_log_paths())[-1]

    def get_log_paths(self):
        return glob.glob('{}*'.format(self._log_path))

class RotatingCacheHandler(logging.handlers.RotatingFileHandler):
    """
    Handler that acts the same as RotatingFileHandler, except it has a data_lost_callback

    callback gets called when doRollover will cause the oldest backup to be overwritten
    """
    def __init__(self, filename, mode='a', maxBytes=0, backupCount=0,
                 encoding=None, delay=False, data_lost_callback=None):
        super().__init__(filename, mode=mode, maxBytes=maxBytes,
            backupCount=backupCount, encoding=encoding, delay=delay)
        self._data_lost_callback = data_lost_callback

    def doRollover(self):
        do_callback = False
        last_backup = self.rotation_filename(
            "%s.%d"%(self.baseFilename, self.backupCount))
        # If the last backup position exists then it will be removed in the rollover
        if os.path.exists(last_backup):
            do_callback = True
        super().doRollover()
        if do_callback:
            if callable(self._data_lost_callback): self._data_lost_callback()


class CacheReader:
    """
    Turns an iterable of cached base64 events into a readable object
    suitable to be passed as a message body of an http post.
    """
    _header = b'data=' + base64.b64encode(b'[  ')
    # Since every entry already contains a comma, we have to stick in
    # one more list entry, and it has to be a technically valid event
    # or the batch will be rejected.  But we explicitly do not want
    # a valid token since we want this to be silently ignored.
    _footer = base64.b64encode(b'{"event":"","properties":{"token":""}}]')

    def __init__(self, iterable):
        self._sent_header = False
        self._iter = iter(iterable)
        self._iter_len = sum(len(x) for x in iterable)

    def read(self, size):
        """
        First return self._header, then every entry in self._iter,
        then self._footer, then an empty string to indicate EOF

        Ignores size because we know that the requested size (8192)
        will always exceed our event length.
        """
        if not self._sent_header:
            self._sent_header = True
            return self._header
        if not self._iter:
            return b''
        try:
            return next(self._iter)
        except StopIteration:
            self._iter = None
            return self._footer

    def __len__(self):
        return self._iter_len + len(self._header) + len(self._footer)

