"""
This module is a collection of utility functions that don't fit somewhere more
specific.

"""
import contextlib
import json
import os
import sys
import time
import subprocess
import urllib.parse
import shutil
import hashlib
import datetime

import kaiten.constants
import kaiten.enum
import kaiten.log

def chroot_path(root_path, path):
    """
    Return path chrooted into root_path, preventing access to any
    directory outside of root_path.
    """
    #TODO(chris): symlinks can still escape...
    return root_path + os.path.abspath('/' + path)

def recursive_update(updatee, updater):
    """
    dict.update will overwrite subdicts.  We need our own custom updater so
    we don't do that.
    """
    for key, val in updater.items():
        if key in updatee and isinstance(updatee[key], dict):
            if isinstance(val, dict):
                recursive_update(updatee[key], updater[key])
            # Don't overwrite a dict with a non-dict
        else:
            updatee[key] = updater[key]

def read_json_file(path, ignore_error=False):
    """ Return the contents of a json file """
    log = kaiten.log.getlogger(object)
    try:
        with open(path) as fp:
            return json.load(fp)
    except Exception:
        if ignore_error:
            log.info('Failed to load %s, continuing.', path)
            return {}
        else:
            # Even though we will likely log this error again, path
            # will not always get logged unless we log it here.
            log.error("Error loading %s", path, exc_info=True)
            raise

@contextlib.contextmanager
def open_write(path):
    """
    Open a file for writing

    Equivalent to open(path, 'w'), but:
      - Enforces use as a context
      - Makes sure the directory containing the file exists
      - Forces a sync to disk when leaving the context
    """
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w') as f:
        yield f
        f.flush()
        os.fsync(f.fileno())

def write_json_file(path, data):
    """ (Over)write a json file """
    with open_write(path) as f:
        json.dump(data, f, indent=4, sort_keys=True)

def _load_configs(base_path, *override_paths):
    config = read_json_file(base_path)
    for path in override_paths:
        # These overrides might not even exist...
        recursive_update(config, read_json_file(path, ignore_error=True))
    return config

def get_config():
    """ Asemble all of our different config files into a single dict """
    config = _load_configs(
        kaiten.constants.config_path,
        kaiten.constants.libmachine_config_path,
        kaiten.constants.extruder_settings_path,
        kaiten.constants.config_home_path,
        kaiten.constants.calibration_settings,
        kaiten.constants.firmware_update_settings,
        kaiten.constants.mixpanel_settings,
    )

    # If config_home_path contains a kaiten subdict, we want to use
    # these values as a kaiten config, and they need to override the
    # values from kaiten_path.  But we still want values from
    # kaiten_home_path to override both of these.
    kaiten_config = read_json_file(kaiten.constants.server_path)
    recursive_update(kaiten_config, config.get("kaiten", {}))
    recursive_update(kaiten_config, read_json_file(
        kaiten.constants.kaiten_home_path, ignore_error=True))

    # For now the kaiten config is still internally a subdict of the
    # machine config, despite our efforts to externally separate them
    config["kaiten"] = kaiten_config
    return config

def fix_kaiten_home_config(config):
    """
    Older firmware versions stored the kaiten config as a subdict
    of the machine settings, and this will remain in the home
    settings after an upgrade.
    """
    config_path = kaiten.constants.config_home_path
    server_path = kaiten.constants.kaiten_home_path
    home_config = read_json_file(config_path, ignore_error=True)
    if 'kaiten' in home_config:
        write_json_file(server_path, config['kaiten'])
        home_config.pop('kaiten')
        if home_config:
            write_json_file(config_path, home_config)
        else:
            os.remove(config_path)

def read_calibration_settings():
    return read_json_file(kaiten.constants.calibration_settings)

def write_calibration_settings(settings):
    write_json_file(kaiten.constants.calibration_settings, settings)

def update_calibration_settings(update_dict):
    log = kaiten.log.getlogger(object)
    if not os.path.isfile(kaiten.constants.calibration_settings):
        log.warning("no calibration settings found, writing defaults")
        write_calibration_settings(update_dict)
    else:
        try:
            current_calibration = read_calibration_settings()
            recursive_update(current_calibration, update_dict)
            write_calibration_settings(current_calibration)
        except Exception:
            log.warning("Exception reading calibration settings, replacing with udpate")
            write_calibration_settings(update_dict)

def read_bytes(file, byte_count):
    """ Iterate through the contents of a file by fixed size blocks """
    data = file.read(byte_count)
    while data:
        yield data
        data = file.read(byte_count)

def copy(src, dst, block_size=32768):
    """
    Like shutil.copy, but yields periodically.  Also does not allow
    dst to be an existing directory.
    TODO: currently this does not copy permission bits
    """
    os.makedirs(os.path.dirname(dst), exist_ok=True)
    # Use a memory view to avoid unnecessary mallocing
    file_buffer = memoryview(bytearray(block_size))
    with open(src, 'rb', buffering=0) as f, open(dst, 'wb', buffering=0) as o:
        yield
        while True:
            n = f.readinto(file_buffer)
            if n == 0: break
            if n:
                o.write(file_buffer[:n])
            yield
        os.fsync(o.fileno())

def sleep(seconds):
    """ A version of time.sleep() that you can yield from in a generator """
    import datetime
    target = datetime.datetime.utcnow() + datetime.timedelta(0, seconds)
    yield
    while datetime.datetime.utcnow() < target:
        yield

def subprocess_call(*args, **kwargs):
    """
    Like subprocess.call, but yield until the subprocess is done.
    """
    import subprocess
    proc = subprocess.Popen(*args, **kwargs)
    while None is proc.poll():
        yield
    return proc.poll()

def await_process(name, alive):
    """
    Yield until a process that contains the name argument is either present in
    (if alive=True) or absent from (if alive=False) the output of ps.

    Note: This process does _not_ do any sleeps internally; you probably should
    not yield from it when you have a fast contract time.
    """
    done = False
    original_pid = None
    while not done:
        ps_output = yield from kaiten.util.subprocess_get_output(['ps'])
        procs = ps_output.split('\n')[1:]
        pid = None
        for line in procs:
            if name in line:
                pid = line.strip().split(' ')[0]
                original_pid = pid
                break
        if alive:
            done = None is not pid
        else:
            done = None is pid
        yield
    return original_pid

def subprocess_get_output(*args, **kwargs):
    """
    Like subprocess.check_output, but yield until the subprocess is done,
    ignore non-zero exit status, and utf-8 decode the output
    """
    import subprocess
    kwargs['stdout'] = subprocess.PIPE
    proc = subprocess.Popen(*args, **kwargs)

    # Set the file decriptor which reads proc's stdout to non-blocking
    import fcntl
    flags = fcntl.fcntl(proc.stdout, fcntl.F_GETFL)
    fcntl.fcntl(proc.stdout, fcntl.F_SETFL, flags | os.O_NONBLOCK)

    out = ''
    while None is proc.poll():
        out_chunk = proc.stdout.read()
        if out_chunk:
            out += out_chunk.decode('utf-8')
        yield
    out_chunk = proc.stdout.read()
    if out_chunk:
        out += out_chunk.decode('utf-8')

    return out

def simple_parallel(main, *subs):
    """
    Execute multiple generators in parallel.  The main generator dictates
    what values get yielded (None is yielded while the main generator is
    done and other generators still remain).
    """
    import itertools
    # zip_longest basically does what we want.  It doesn't have send(),
    # throw() or close() though.  It also doesn't close() its arguments.
    super_gen = itertools.zip_longest(main, *subs)
    try:
        for results in super_gen:
            yield results[0]
    finally:
        def do_close(gen):
            try: gen.close()
            except Exception: pass
        do_close(main)
        for sub in subs:
            do_close(subs)

def play_buzzer(song):
    """
    Don't invoke this directly, since this utility doesn't check to see
    if the user has disabled sound.  Use server.play_buzzer() instead.
    """
    songs = kaiten.enum.enum(
        "buzzer_songs",
        startup=0,
        start_print=1,
        print_done=2,
        notify=3,
        error=4,
        test_tones=5,
    )
    song_nr = getattr(songs, song)

    if not kaiten.constants.mock:
        with open('/dev/buzzer_dv', 'w') as f:
            f.write(str(song_nr)+'\n')

def process_sequence(filename, config, environment, handler, log=None):
    """
    filename is the name of a json file in /usr/scripts/
    environment is a dict of variables for substitution
    handler is a function that takes (func_name, *args, **kwargs)
    """
    # parse the sequence file into a json dict
    custom_script = os.path.join(kaiten.constants.primary_script_dir, filename)
    base_script = os.path.join(kaiten.constants.secondary_script_dir, filename)
    sequence_dict = read_json_file(custom_script, ignore_error=True)
    if not sequence_dict:
        sequence_dict = read_json_file(base_script)

    # override some environment variables
    environment.update(sequence_dict.get("environment", {}))

    def convert_param(param, dict_in):
        """
        If this param is a string and a variable, looks it up in the
        environment and returns it.  We raise a KeyError if this
        variable doens't exist in the environmen.

        To support load_temperature_settings, this function needs to be
        recursive :(
        """
        if isinstance(param, list):
            param = [convert_param(p, dict_in) for p in param]
        elif isinstance(param, dict):
            for key, val in param.items():
                dict_in[key] = convert_param(val, dict_in)
            param = None
        elif isinstance(param, str) and param.startswith("$"):
            old = param
            param = param.lstrip("$")
            param = environment.get(param)
            if log:
                log.info("replaced {0} with {1}".format(old, param))
        return param

    for _list in sequence_dict["sequence"]:
        if log:
            log.info("Processing item: {0}".format(_list))
        func_name = _list[0]
        params = _list[1:]
        # We convert params to a list so we can use it multiple times
        # (map objects are iterators)
        kwparams = {}
        params = [convert_param(p, kwparams) for p in params]
        params = [p for p in params if None is not p]
        yield from handler(func_name, *params, **kwparams)

class Version(object):
    """ Object representing a firmware version """
    def __init__(self, major, minor, bugfix, build=None,
                 release_type=None, builder=None, custom_string=None, *args):
        self.major = int(major)
        self.minor = int(minor)
        self.bugfix = int(bugfix)
        self._vers = (self.major, self.minor, self.bugfix)
        # These meta values should only be used for user-facing strings
        # and no logic should be done with them.
        self.build = int(build) if build is not None else None
        self.release_type = release_type
        self.builder = builder
        self.custom_string = custom_string

    @staticmethod
    def from_str(s):
        # Constructor will try to conver to int what is we need as int
        return Version(*map(str,s.split('.')))

    @staticmethod
    def from_dict(d):
        return Version(**d)

    @staticmethod
    def from_list(l):
        # BW-2705 introduced firmware version metadata that can be
        # packed as a list (e.g. manifests list on AWS). A firmware version
        # can be represented by 3 to 7 parts, last 4 are optional.
        if 3 <= len(l) <= 7:
            return Version(*l)
        elif len(l) > 7:
            return Version(*l[0:6])
        else:
            raise ValueError('Not a version')

    @staticmethod
    def factory(x):
        if isinstance(x, Version): return x
        if isinstance(x, str): return Version.from_str(x)
        if isinstance(x, dict): return Version.from_dict(x)
        if isinstance(x, list): return Version.from_list(x)
        raise ValueError('Not a version')

    def _cmp(self, other):
        if self._vers == other._vers:
            # Comparing build numbers: If a build is a jenkins build,
            # it will have a build number. We want people to be on
            # Jenkins builds, so we want to prioritize build numbers.
            # So, we compare build numbers, and consider no build number
            # at all to always be inferior to any build number.
            if self.build is not None and other.build is None: return 1
            elif self.build is None and other.build is not None: return -1
            elif self.build is None and other.build is None: return 0
            else:
                if self.build > other.build: return 1
                elif self.build < other.build: return -1
                else: return 0
        elif self._vers > other._vers: return 1
        elif self._vers < other._vers: return -1
        else:
            raise RuntimeError("Failed to compare versions {0} and {1}"\
                               .format(str(self), str(other)))


    def __lt__(self, other): return self._cmp(Version.factory(other)) < 0
    def __gt__(self, other): return self._cmp(Version.factory(other)) > 0
    def __le__(self, other): return self._cmp(Version.factory(other)) <= 0
    def __ge__(self, other): return self._cmp(Version.factory(other)) >= 0
    def __eq__(self, other): return self._cmp(Version.factory(other)) == 0
    def __ne__(self, other): return self._cmp(Version.factory(other)) != 0

    def __str__(self):
        vers = [str(x) for x in self._vers]
        if self.build is not None:
            vers.append(str(self.build))
        if self.release_type is not None:
            vers.append(self.release_type)
        if self.builder is not None:
            vers.append(self.builder)
        if self.custom_string is not None:
            vers.append(self.custom_string)
        return '.'.join(vers)

    def safe_str(self):
        """
        A string version with a consisently parsable format

        Because we use string formatted version objects as part of
        the firmware upload process, we need to have a format that
        remains parseable by all firmware versions, even as we
        continue to add features to this firmware version object.
        Major.minor.bugfix is understood by all firmware versions
        that try to look at the previous firmware's version, and
        is also all we need to convey whether the older firmware
        contains any given bug that the the new firmware might need
        to correct.
        """
        return '.'.join(str(x) for x in self._vers)

    def __iter__(self):
        # Makes dict(self) do what we want it to
        yield ('major', self.major)
        yield ('minor', self.minor)
        yield ('bugfix', self.bugfix)
        if self.build is not None:
            yield ('build', self.build)
        if self.release_type is not None:
            yield ('release_type', self.release_type)
        if self.builder is not None:
            yield ('builder', self.builder)
        if self.custom_string is not None:
            yield ('custom_string', self.custom_string)

def get_fw_version():
    """ Get the current firmware version number """
    global _fw_version
    try:
        return _fw_version
    except NameError:
        fw_dict = read_json_file(kaiten.constants.version_path)
        _fw_version = Version.from_dict(fw_dict["firmware_version"])
        return _fw_version


def get_mac_address():
    """ Get the MAC address of the bot """
    # DO NOT try to read from the nor flash here
    return os.environ.get('eth', '00:00:00:00:00:00')

def seconds_to_hours(seconds):
    return float(seconds)/60/60

def get_serial_dict():
    global _printer_serial
    try:
        return _printer_serial
    except NameError:
        _printer_serial = read_json_file(
            '/var/machine_settings/serial.json', ignore_error=True)
        return _printer_serial

def get_firmware_path(firmware_filename):
    return os.path.join(kaiten.constants.home_dir, "firmware", firmware_filename)

class FileDownload:
    def __init__(self, server, url, filepath):
        self._log = kaiten.log.getlogger(self)
        self._server = server
        parsed = urllib.parse.urlparse(url)
        self._url = urllib.parse.urlunparse((parsed[0],
                                            parsed[1],
                                            urllib.parse.quote(parsed[2]),
                                            parsed[3],
                                            parsed[4],
                                            parsed[5]))
        self._filepath = filepath
        self.complete = False
        self.error = False
        self._request = None
        self._writer = None
        self._success_callback = None
        self._error_callback = None

    def set_success_callback(self, callback):
        self._success_callback = callback

    def set_error_callback(self, callback):
        self._error_callback = callback

    def _success_response(self, response):
        self.close()
        self.complete = True
        if self._success_callback: self._success_callback()

    def _error_response(self, **kwargs):
        self._log.error("Error downloading file {0}: {1}".format(self._url, kwargs))
        self.close()
        self.error = True
        self.complete = True
        if self._error_callback: self._error_callback()

    def progress(self):
        if self._request:
            return self._request.progress()
        else:
            return 0

    def start(self):
        self._log.info("Starting file download: {0}".format(self._url))
        os.makedirs(os.path.dirname(self._filepath), exist_ok=True)
        self._writer = open(self._filepath, "wb")
        url = urllib.parse.urlparse(self._url)
        path = "{0}?{1}".format(url.path, url.query)
        self._request = self._server.http_request(
            url.netloc,
            path,
            "GET",
            params={},
            https=(url.scheme == "https"),
            token=None,
            success_callback=self._success_response,
            error_callback=self._error_response,
            file_response=self._writer,
            timeout=60,
        )

    def close(self):
        # Ensures file descriptor gets closed
        if self._writer is not None and not self._writer.closed:
            self._writer.close()

def remove_dir(root):
    """
    A generator that completely removes a directory.  Since removing a
    directory could potentially take some time, we don't want to do it
    in one blocking call and lock up kaiten.
    """
    # Do the hard legwork first and remove all the larger files
    # All dirs is a list of directories, with the lower level ones at the front
    all_dirs = []
    for _tuple in os.walk(root):
        (dirname, dirs, files) = _tuple
        for _file in files:
            yield os.unlink(os.path.join(root, dirname, _file))
        all_dirs.insert(0, os.path.join(root, dirname))
    # Now remove the dirs
    for _dir in all_dirs:
        yield shutil.rmtree(_dir)

def remove_prefix(prefix):
    """
    A generator that removes all files matching a given prefix

    For example, the prefix /logs/spam.log would remove /logs/spam.log,
    /log/spam.log.1, /log/spam.log.2, and so on.
    """
    dir_name, file_prefix = os.path.split(prefix)
    files = os.listdir(dir_name)
    for file in files:
        if file.startswith(file_prefix):
            os.unlink(os.path.join(dir_name, file))
            yield

def iterative_copytree(src, dst, symlinks=False):
    """
    An iterative copytree function.
    This source was ripped directly from shutil's implementation, and turned
    into a generator for kaiten's use
    """
    names = os.listdir(src)
    os.makedirs(dst)
    errors = []
    for name in names:
        srcname = os.path.join(src, name)
        dstname = os.path.join(dst, name)
        try:
            if symlinks and os.path.islink(srcname):
                linkto = os.readlink(srcname)
                os.symlink(linkto, dstname)
            elif os.path.isdir(srcname):
                yield from self._iterative_copytree(srcname, dstname, symlinks)
            else:
                shutil.copy2(srcname, dstname)
                yield
                # Sync the new file, ensure it is written to disk
                with open(dstname, 'a+') as fh:
                    fh.flush()
                    os.fsync(fh.fileno())
                yield
        # catch the Error from the recursive copytree so that we can
        # continue with other files
        except shutil.Error as err:
            errors.extend(err.args[0])
            yield
        # XXX What about devices, sockets etc.?
        except OSError as why:
            errors.append((srcname, dstname, str(why)))
            yield
    try:
        shutil.copystat(src, dst)
        yield
    except OSError as why:
        errors.extend((src, dst, str(why)))
        yield
    if errors:
        raise shutil.Error(errors)


class HashChecker(object):
    def __init__(self, hashfunc, files, result_callback):
        self._log = kaiten.log.getlogger(self)
        self.hashfunc = hashfunc
        self.files = files
        self.result_callback = result_callback
        self._generator = self.check_files_hash()

    def check_files_hash(self):
        """ checks a list of files with given hashes and calls the callback with
        a list of filenames that did not match
        @param pointer to the hashfunc constructor to use ie (hashlib.md5)
        @param files,list of objects containing a 'filepath' and 'checksum' members
        @param result_callback function to call when complete, will return a list
            of filenames that don't match their checksums
        """
        mismatched_files = []
        for f in self.files:
            f_hashfunc = self.hashfunc()
            with open(f['filename'], mode='rb') as fp:
                for buf in kaiten.util.read_bytes(fp, 65536):
                    f_hashfunc.update(buf)
                    yield
            file_checksum = f_hashfunc.hexdigest()
            if file_checksum != f['checksum']:
                self._log.error(f['filename'] + " checksum " + f['checksum'] + " didn't match calculated " + file_checksum )
                mismatched_files.append(f['filename'])
            yield
        self.result_callback(mismatched_files)
        raise StopIteration

    def contract_duration(self):
        return datetime.timedelta(0,0,1)

    def expected_run_time(self):
        return datetime.timedelta(seconds=kaiten.constants.normal_generator_time)

    def __next__(self):
        next(self._generator)
